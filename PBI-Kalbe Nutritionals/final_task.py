# -*- coding: utf-8 -*-
"""Final Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/171WkuryxmWyUw4TJbtdOt_RmW4cGCOZ7

#Import Library
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

"""#Import Data"""

df_customer = pd.read_csv('/content/Case Study - Customer.csv', delimiter=';')
df_product = pd.read_csv('/content/Case Study - Product.csv', delimiter=';')
df_store = pd.read_csv('/content/Case Study - Store.csv', delimiter=';')
df_transaction = pd.read_csv('/content/Case Study - Transaction.csv', delimiter=';')

"""#Data Preprocessing

##Data Customer
"""

df_customer.head()

print('Daftar tipe data:\n' + str(df_customer.dtypes))

df_customer['Income'] = df_customer['Income'].replace(',', '.', regex=True)
df_customer['Income'] = df_customer['Income'].astype(float)
print('Daftar tipe data:\n' + str(df_customer.dtypes))

df_customer.info()

df_customer.describe()

df_customer.isna().sum()

df_customer.dropna(inplace=True)
df_customer.isna().sum()

df_customer.duplicated().sum()

"""##Data Transaction"""

df_transaction.head()

print('Daftar tipe data:\n' + str(df_transaction.dtypes))

df_transaction['Date'] = pd.to_datetime(df_transaction['Date'])
print('Daftar tipe data:\n' + str(df_transaction.dtypes))

df_transaction.head()

df_transaction.describe()

df_transaction.info()

df_transaction.isna().sum()

df_transaction.duplicated().sum()

"""##Data Product"""

df_product.head()

print('Daftar tipe data:\n' + str(df_product.dtypes))

df_product.info()

df_product.describe()

df_product.isna().sum()

df_product.duplicated().sum()

"""##Data Store"""

df_store.head()

print('Daftar tipe data:\n' + str(df_store.dtypes))

df_store.info()

df_store[['Latitude','Longitude']] = df_store[['Latitude','Longitude']].replace(',', '.', regex=True)
df_store[['Latitude','Longitude']] = df_store[['Latitude','Longitude']].astype(float)
print('Daftar tipe data:\n' + str(df_store.dtypes))

df_store.head()

df_store.describe()

df_store.isna().sum()

df_store.duplicated().sum()

"""##Merge Dataset"""

df_customer.head()

df_transaction.head()

df_product.head()

df_store.head()

df_merge = df_transaction.merge(df_customer, on='CustomerID')

df_merge = df_merge.merge(df_product.drop(columns='Price'), on='ProductID')

df_merge

df_merge = df_merge.merge(df_store, on='StoreID')

df_merge.head()

"""#Modeling

##Regresi with ARIMA

###Merge Dataframe
"""

df_merge

df_regresi = df_merge.groupby(['Date']).agg({'Qty':'sum'}).reset_index()
df_regresi = df_regresi.set_index('Date')

"""###Cek Seasonal Decompose"""

from statsmodels.tsa.seasonal import seasonal_decompose
decomp = seasonal_decompose(df_regresi)

decomp.plot()
plt.gcf().set_size_inches(10, 5)
plt.show()

plt.figure(figsize=(10,10))
plt.subplot(311)
decomp.trend.plot()
plt.title('Trend')
plt.subplot(312)
decomp.seasonal.plot()
plt.title('Seasonal')
plt.subplot(313)
decomp.resid.plot()
plt.title('Resid')

"""dapat dilihat dari grafik-grafik diatas, data yang dimiliki meurpakan data seasonal

###Cek Stasioner
"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_regresi)
print('ADF Statistic: %f' % result[0])
print('p-value: %f' % result[1])
print('Critical Values:')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key, value))

"""Merupakan data stasioner karena p-value kurang dari alpha atau 0.005

###Memecah jadi data latih dan data uji
"""

cutOff = round(df_regresi.shape[0]*0.8)
df_train = df_regresi[:cutOff]
df_test = df_regresi[cutOff:]
df_train.shape, df_test.shape

df_train

df_test

"""###Mencari nilai p,d,dan q"""

import numpy as np, pandas as pd
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# Original Series
fig, (ax1, ax2, ax3) = plt.subplots(3)
ax1.plot(df_regresi); ax1.set_title('Original Series'); ax1.axes.xaxis.set_visible(False)
# 1st Differencing
ax2.plot(df_regresi.diff()); ax2.set_title('1st Order Differencing'); ax2.axes.xaxis.set_visible(False)
# 2nd Differencing
ax3.plot(df_regresi.diff().diff()); ax3.set_title('2nd Order Differencing')
plt.show()

from statsmodels.graphics.tsaplots import plot_acf
fig, (ax1, ax2, ax3) = plt.subplots(3)
plot_acf(df_regresi, ax=ax1)
plot_acf(df_regresi.diff().dropna(), ax=ax2)
plot_acf(df_regresi.diff().diff().dropna(), ax=ax3)

"""Dari kedua grafik diatas yaitu ordering dan autocorelation, nilai d yang didapatkan adalah 0 karena setelah dilakukan ordering pertama lag berpindah ke negatif. Hal ini terjadi karena data termasuk stasioner."""

from statsmodels.graphics.tsaplots import plot_pacf
plot_pacf(df_regresi.diff().dropna())

"""Dari grafik partial correlation diatas didapatkan nilai p yaitu 4. Hal ini dilihat dari lag pertama sampai ketiga yang berbeda jauh dari signifikan dan lag ke 4 yang hanya berbeda sedikit."""

plot_acf(df_regresi.diff().dropna())

"""Disini bisa terlihat terdapat 2 flag yang sudah jauh dari jangkuan signfikan sehingga daidaptkan nilai q yaitu 2.

###Modeling
"""

import statsmodels.api as sm
import sklearn.metrics as metrics

"""#####Menggunakan seluruh data"""

model = sm.tsa.arima.ARIMA(df_regresi, order=(4,0,2))
result = model.fit()
result.summary()

plt.plot(df_regresi)
plt.plot(result.predict(1, 365), color="red")
plt.show()

"""####Prediksi menggunakan data latih dan diuji dengan data uji"""

y=df_train['Qty']
ARIMAmod = sm.tsa.arima.ARIMA(y, order=(4,0,2))
ARIMAmod = ARIMAmod.fit()

y_pred = ARIMAmod.get_forecast(len(df_test))
df_y_pred = y_pred.conf_int()
df_y_pred['predict'] = ARIMAmod.predict(start=df_y_pred.index[0], end=df_y_pred.index[-1])
df_y_pred.index = df_test.index
y_pred_fin = df_y_pred['predict']
mae = metrics.mean_absolute_error(y_pred_fin, df_test['Qty'])
mse = metrics.mean_squared_error(y_pred_fin, df_test['Qty'])
rmse = np.sqrt(mse) # or mse**(0.5)
print("RMSE :", rmse)
print("MAE :", mae)

plt.figure(figsize=(10,5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_fin, color='black', label='ARIMA Predcition')
plt.legend()

"""hasil prediksi dari model ARIMA yang telah dibuat bisa dibilang masih belum cukup bagus karena nilai RMSE yang berniali 15 lebih dan MAE yang bernilai 12 lebih."""

y_pred = ARIMAmod.get_forecast(len(df_test)+60)
df_y_pred = y_pred.conf_int()
df_y_pred['predict'] = ARIMAmod.predict(start=df_y_pred.index[0], end=df_y_pred.index[-1])
y_pred_fin = df_y_pred['predict']

plt.figure(figsize=(10,5))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_fin, color='black', label='ARIMA Predcition')
plt.legend()

"""Dan prediksi untuk jumlah quantity untuk dua bulan kedepan berdasarkan model ARIMA yang telah dibuat yaitu sekitar 50. Tentunya prediksi tersebut bisa dibilang belum cukup baik dan meyakinkan mengingat hasil pengujian yang belum memuaskan.

####Prediksi menggunakan Seasonal Arima
"""

SARIMAmod = sm.tsa.statespace.SARIMAX(y, order=(4,0,2), seasonal_order=(1,1,1,12))
SARIMAmod = SARIMAmod.fit()

y_pred = SARIMAmod.get_forecast(len(df_test))
df_y_pred = y_pred.conf_int()
df_y_pred['predict'] = SARIMAmod.predict(start=df_y_pred.index[0], end=df_y_pred.index[-1])
df_y_pred.index = df_test.index
y_pred_fin = df_y_pred['predict']
mae = metrics.mean_absolute_error(y_pred_fin, df_test['Qty'])
mse = metrics.mean_squared_error(y_pred_fin, df_test['Qty'])
rmse = np.sqrt(mse) # or mse**(0.5)
r2 = metrics.r2_score(y_pred_fin, df_test['Qty'])
print("RMSE :", rmse)
print("MAE :", mae)
print("MSE :", mse)

plt.figure(figsize=(10,10))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_fin, color='black', label='SARIMA Predcition')
plt.legend()

"""Penggunanan model seasonal ARIMA terlihat lebih baik diabandingkan model ARIMA meskpiun nilai RMSE dan MAE tidak terlihat begitu jauh."""

y_pred = SARIMAmod.get_forecast(len(df_test)+60)
df_y_pred = y_pred.conf_int()
df_y_pred['predict'] = SARIMAmod.predict(start=df_y_pred.index[0], end=df_y_pred.index[-1])
y_pred_fin = df_y_pred['predict']

plt.figure(figsize=(20,10))
plt.plot(df_train['Qty'])
plt.plot(df_test['Qty'], color='red')
plt.plot(y_pred_fin, color='black', label='SARIMA Predcition')
plt.legend()

"""Hasil prediksi jumlah kauntitas atau jumlah barang untuk 2 bulan kedepan dengan menggunakan model seasonal ARIMA atau SARIMA adalah sekitar 50-55 barang. Tentunya prediksi yang dihasilkan dari model ini bisa dibilang lebih terpercaya dibanding model ARIMA sebelumnya dilihat dari hasil pengujian yang telah dilakukan walau peningkatannya tidak terlalu signifikan.

##Clustering dengab KMeans

###Preprocessing
"""

df_merge.head()

df_cluster = df_merge.groupby(['CustomerID']).agg({'TransactionID':'count','Qty':'sum','TotalAmount':'sum'}).reset_index()
# df_cluster = df_cluster.set_index('CustomerID')
df_cluster.drop(columns='CustomerID', inplace=True)
df_cluster.rename(columns = {'TransactionID':'Count_Transaction','Qty':'Total_Quantity','TotalAMount':'Sum_TotalAmount'}, inplace = True)
df_cluster.head()

def print_rentang(df_input):
  list_fitur = df_input
  for fitur in list_fitur:
    max = df_input[fitur].max()
    min = df_input[fitur].min()
    print('Rentang fitur ',fitur,' adalah ',max-min)

def minmax (df_input):
  df_input = df_input.copy()
  list_fitur = df_input
  for fitur in list_fitur:
    max = df_input[fitur].max()
    min = df_input[fitur].min()
    df_input[fitur] = (df_input[fitur]-min)/(max-min)
  return df_input

print_rentang(df_cluster)

df_cluster = minmax(df_cluster)
df_cluster.describe()

print_rentang(df_cluster)

df_cluster.head()

"""###Find Optimal k value with elbow method"""

from sklearn.cluster import KMeans

distortions = []
K = range(1,10)
for k in K:
    kmeanModel = KMeans(n_clusters=k)
    kmeanModel.fit(df_cluster)
    distortions.append(kmeanModel.inertia_)

plt.figure(figsize=(16,8))
plt.plot(K, distortions, 'bx-')
plt.xlabel('k')
plt.ylabel('Distortion')
plt.title('The Elbow Method showing the optimal k')
plt.show()

"""Dari plot yang sudah kita buat, dapat dilihat pada nilai k=3, grafik mulai menjadi sangat landai sehingga k optimal adalah 3

###Modeling
"""

kmeanModel = KMeans(n_clusters=3)
kmeanModel.fit(df_cluster)
cluster = kmeanModel.predict(df_cluster)
df_cluster['Cluster'] = cluster

df_cluster.head()

df_int = df_cluster.groupby(['Cluster']).agg({'Count_Transaction':'count','Total_Quantity':'sum','TotalAmount':'sum'}).reset_index()
df_int.head()

"""dapat kita lihat dari hasil clustering yang sudah dilakukan terdapat 3 cluster yang telah terbentuk.

cluster-cluster tersebut dapat kita bedakan dengan mengamati pada kluster pertama merupakan cluster yang sering melakukan transaksi dan menghasilkan keuntungan yang paling besar. cluster selanjutnya merupakan cluster yang sering melakukan transaksi namun tidak sesering cluster pertama dan keuntungan yang didapatkan juga paling sedikit dikarenakan barang yang dibeli juga tidak terlalu banyak dilihat dari total quantitas. cluster yang terakhir merupakan cluster dengan jumlah transaksi paling sedikit namun menghasilkan keuntungan yang lebih besar daripada cluster kedua dan jumlah barang yang dibeli juga lebih banyak dibandingkan cluster kedua.

Untuk itu, hal yang dapat kita lakukan adalah dengan mempromosikan produk yang lebih menghasilkan profit pada cluster pertama dengan memberikan promo-promo yang bergandengan seperti memberikan promo pada pembelian produk yang sering dibeli dengan produk yang menghasilkan profit paling banyak.

Untuk cluster 2 kita bisa memberikan promo seperti 2 gratis 1 mengingat cluster ini sering melakukan pembelian namun jumlah barang yang dibeli tidak terlalu banyak

Untuk cluster 3, dapat dilihat mereka tidak terlalu sering melakukan transaksi namun jumlah barang yang dibeli cukup banyak sehingga menghasilkan keuntungan yang cukup banyak juga. Untuk itu kita bisa menawarkan prodk-produk baru serta meberikan diskon yang terus berubah-ubah dalam jangka waktu tertentu sehingga mereka lebih sering membeli untuk mengincar diskon-diskon yang berbeada-beda
"""